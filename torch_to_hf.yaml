protocolVersion: 2
name: torch_to_hf
type: job
jobRetryCount: 0
prerequisites:
  - type: dockerimage
    uri: >-
      bench.azurecr.io/superbench:sigma-megatron-with-msccl-large-scale-opt-nccl-2.23
    name: docker_image0
taskRoles:
  worker:
    instances: 1
    completion:
      minFailedInstances: 1
    taskRetryCount: 0
    dockerImage: docker_image0
    extraContainerOptions:
      shmMB: 16384
      infiniband: true
    resourcePerInstance:
      gpu: 8
      cpu: 88
      memoryMB: 819200
    commands:
      - echo "Prepare NCCL env"
      - mkdir -p /opt/microsoft
      - >-
        wget
        https://raw.githubusercontent.com/Azure/azhpc-images/master/topology/ndv4-topo.xml
        -O /opt/microsoft/ndv4-topo.xml
      - export NCCL_IB_PCI_RELAXED_ORDERING=1
      - export NCCL_SOCKET_IFNAME=eth0
      - export CUDA_DEVICE_ORDER=PCI_BUS_ID
      - export NCCL_NET_GDR_LEVEL=5
      - export NCCL_TOPO_FILE=/opt/microsoft/ndv4-topo.xml
      - export NCCL_DEBUG=WARN
      - nvidia-smi
      - echo "Download code"
      - git clone https://github.com/SteveKGYang/Megatron-LLM.git
      - cd Megatron-LLM
      - ls -alh .
      - echo "Prepare other envs"
      - pip install transformers
      - pip install h5py
      - pip install wandb
      - echo "[Used Envs]"
      - pip list
      - export AZUREML_NODE_COUNT=$PAI_TASK_ROLE_TASK_COUNT_worker
      - export GPUS_PER_NODE=8
      - export MASTER_ADDR=$PAI_HOST_IP_worker_0
      - export MASTER_PORT=$PAI_PORT_LIST_worker_0_http
      - export NUM_NODES=$PAI_TASK_ROLE_TASK_COUNT_worker
      - export NODE_RANK=$PAI_CURRENT_TASK_ROLE_CURRENT_TASK_INDEX
      - 'export LD_LIBRARY_PATH=/opt/hpcx/ucx/lib:$LD_LIBRARY_PATH'
      # - echo "Copy data"
      # - mkdir /scratch/
      # - mkdir /scratch/proof_pile_2-algebraic_stack
      # - chmod 777 /scratch/proof_pile_2-algebraic_stack
      # - mkdir /scratch/proof_pile_2-open_web_math
      # - chmod 777 /scratch/proof_pile_2-open_web_math
      # - mkdir /scratch/redpajama-arxiv
      # - chmod 777 /scratch/redpajama-arxiv
      # - mkdir /scratch/wiki
      # - chmod 777 /scratch/wiki
      # - mkdir /scratch/dclm_data
      # - chmod 777 /scratch/dclm_data
      # - mkdir /scratch/starcoder
      # - chmod 777 /scratch/starcoder
      # - mkdir /scratch/pes2o
      # - chmod 777 /scratch/pes2o
      # - bash copy_dataset_olmo2.sh
      - echo "Start converting..."
      - python weights_conversion/megatron_to_hf.py --input_dir /mnt/pvc-blob-nfs/klyang/converted_model_edu/ --num_output_shards 2 --output_dir /mnt/pvc-blob-nfs/klyang/hf_converted_model_edu/ 
      # - python weights_conversion/megatron_to_hf.py --input_dir /home/pretraining/klyang/mount_dir/eu_mount/converted_model_edu --num_output_shards 2 --output_dir ./hf_converted_model_edu/
      - echo "End converting..."
defaults:
  virtualCluster: default
extras:
  com.microsoft.pai.runtimeplugin:
    - plugin: ssh
      parameters:
        jobssh: true
        userssh:
          type: custom
          value: ''
    - plugin: teamwise_storage
      parameters:
        storageConfigNames:
          - pvc-blob-nfs
  hivedScheduler:
    jobPriorityClass: prod
    taskRoles:
      worker:
        skuNum: 8
        skuType: A100
  jobStatusChangeNotification:
    running: true
    succeeded: true
    failed: true
